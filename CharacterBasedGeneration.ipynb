{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJRhl22NdNFA"
   },
   "source": [
    "## Training a character language model and studying various ways of generating text\n",
    "\n",
    "**Author: matthieu.labeau@telecom-paris.fr**\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- We will train a network to predict a next character given an input sequence of characters, and use it to generate new sequences.\n",
    "- We will strictly work with local (and not structured, meaning we will only one character at a time) prediction - however, we will look into a relatively simple heuristic to improve the \"structure\": *beam search*. We can also try to improve generation with other methods: *temperature* sampling, *top-k* sampling, *top-p* sampling,\n",
    "- We will use ```keras```to build the model based on a **recurrent neural network** called a **LSTM**, which will use simple features (one-hot vector representing previous characters) to predict the next characters. We will use a small model to avoid training for too long. *Remark: you don't need to know how this model works - just its inputs and outputs !*\n",
    "- We will use a small dataset (poetry, from project Gutenberg) - you can use any data you prefer, as long as you are able to train the model on it.\n",
    "- Even with a small dataset and a small model, training may be long. If you can use a computing infrastructure, like Google colab, it may be more practical - and you probably can obtain better results by using a bigger model and a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JfjG_pve1Ds"
   },
   "source": [
    "#### Obtaining the data\n",
    "- We download directly the ebook from project Gutenberg - you can get any other text you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BCd_FKg1chKU"
   },
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "url = 'http://www.gutenberg.org/cache/epub/6099/pg6099.txt'\n",
    "path = get_file('pg6099.txt', origin=url)\n",
    "\n",
    "f = open(path, 'r' , encoding = 'utf8')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "\n",
    "start = False\n",
    "for line in lines:\n",
    "    if(\"*** START OF THE PROJECT GUTENBERG EBOOK LES FLEURS DU MAL ***\" in line and start==False):\n",
    "        start = True\n",
    "    if(\"            *** END OF THE PROJECT GUTENBERG EBOOK LES FLEURS DU MAL ***\" in line):\n",
    "        break\n",
    "    if(start==False or len(line) == 0):\n",
    "        continue\n",
    "    text.append(line)\n",
    "\n",
    "f.close()\n",
    "text = \" \".join(text)\n",
    "voc_chars = sorted(set([c for c in text]))\n",
    "nb_chars = len(voc_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xJt52pNLFcT",
    "outputId": "77e2df8b-36cd-4dc9-d948-82c74c557bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '5', '6', '7', '8', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '«', '»', 'È', 'É', 'Ï', 'à', 'â', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ï', 'ô', 'ù', 'û', 'ü']\n"
     ]
    }
   ],
   "source": [
    "print(voc_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrHl4Y3SEeWB",
    "outputId": "3a55fdd2-740e-4d17-d9b6-237a00a5fe1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s plaintes,\n",
      "   Ces extases, ces cris, ces pleurs, ces _Te Deum,_\n",
      "   Sont un écho redit par mille labyrinthes;\n",
      "   C'est pour les coeurs mortels un divin opium.\n",
      " \n",
      "   C'est un cri répété par mille sentinelles,\n",
      "   Un ordre renvoyé par mille porte-voix;\n",
      "   C'est un phare allumé sur mille citadelles,\n",
      "   Un appel de chasseurs perdus dans les grands bois!\n",
      " \n",
      "   Car c'est vraiment, Seigneur, le meilleur témoignage\n",
      "   Que nous puissions donner de notre dignité\n",
      "   Que cet ardent sanglot qui roule d'âge en âge\n",
      "   Et vient mourir au bord de votre éternité!\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   LA MUSE VENALE\n",
      " \n",
      " \n",
      "   O Muse de mon coeur, amante des palais,\n",
      "   Auras-tu, quand Janvier lâchera ses Borées,\n",
      "   Durant les noirs ennuis des neigeuses soirées,\n",
      "   Un tison pour chauffer tes deux pieds violets?\n",
      " \n",
      "   Ranimeras-tu donc tes épaules marbrées\n",
      "   Aux nocturnes rayons qui percent les volets?\n",
      "   Sentant ta bourse à sec autant que ton palais,\n",
      "   Récolteras-tu l'or des voûtes azurées?\n",
      " \n",
      "   Il te faut, pour gagner ton pain de chaque\n"
     ]
    }
   ],
   "source": [
    "print(text[20000:21000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN-Cq6PGe61b"
   },
   "source": [
    "#### Keeping track of possible characters\n",
    "- Using a ```set```, create a sorted list of possible characters\n",
    "- Create two dictionnaries, having characters and corresponding indexes as {key: value}, and reverse.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "chars = [a, b, c]\n",
    "```\n",
    "\n",
    "```python\n",
    "chars_indices = {a: 0, b: 1, c: 2}\n",
    "```\n",
    "\n",
    "```python\n",
    "indices_chars = {0: a, 1: b, 2: c}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdbvrYqHe9cg",
    "outputId": "5c08efad-34be-40f0-8bf8-0557d0f5877e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 160503\n",
      "Total number of characters: 92\n"
     ]
    }
   ],
   "source": [
    "print('Corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Total number of characters:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIYxZf1HLFcZ",
    "outputId": "52361a4c-f204-440c-a812-2288c0efbb72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '5': 13, '6': 14, '7': 15, '8': 16, ':': 17, ';': 18, '?': 19, 'A': 20, 'B': 21, 'C': 22, 'D': 23, 'E': 24, 'F': 25, 'G': 26, 'H': 27, 'I': 28, 'J': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'Q': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'V': 41, 'W': 42, 'X': 43, 'Y': 44, '[': 45, ']': 46, '_': 47, 'a': 48, 'b': 49, 'c': 50, 'd': 51, 'e': 52, 'f': 53, 'g': 54, 'h': 55, 'i': 56, 'j': 57, 'k': 58, 'l': 59, 'm': 60, 'n': 61, 'o': 62, 'p': 63, 'q': 64, 'r': 65, 's': 66, 't': 67, 'u': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73, '«': 74, '»': 75, 'È': 76, 'É': 77, 'Ï': 78, 'à': 79, 'â': 80, 'ç': 81, 'è': 82, 'é': 83, 'ê': 84, 'ë': 85, 'î': 86, 'ï': 87, 'ô': 88, 'ù': 89, 'û': 90, 'ü': 91}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: '*', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '5', 14: '6', 15: '7', 16: '8', 17: ':', 18: ';', 19: '?', 20: 'A', 21: 'B', 22: 'C', 23: 'D', 24: 'E', 25: 'F', 26: 'G', 27: 'H', 28: 'I', 29: 'J', 30: 'K', 31: 'L', 32: 'M', 33: 'N', 34: 'O', 35: 'P', 36: 'Q', 37: 'R', 38: 'S', 39: 'T', 40: 'U', 41: 'V', 42: 'W', 43: 'X', 44: 'Y', 45: '[', 46: ']', 47: '_', 48: 'a', 49: 'b', 50: 'c', 51: 'd', 52: 'e', 53: 'f', 54: 'g', 55: 'h', 56: 'i', 57: 'j', 58: 'k', 59: 'l', 60: 'm', 61: 'n', 62: 'o', 63: 'p', 64: 'q', 65: 'r', 66: 's', 67: 't', 68: 'u', 69: 'v', 70: 'w', 71: 'x', 72: 'y', 73: 'z', 74: '«', 75: '»', 76: 'È', 77: 'É', 78: 'Ï', 79: 'à', 80: 'â', 81: 'ç', 82: 'è', 83: 'é', 84: 'ê', 85: 'ë', 86: 'î', 87: 'ï', 88: 'ô', 89: 'ù', 90: 'û', 91: 'ü'}\n"
     ]
    }
   ],
   "source": [
    "print(char_indices)\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzqwN_5gfEcQ"
   },
   "source": [
    "#### Creating training data\n",
    "- We will represent characters using *one-hot vectors*. Hence, the i-th character of n possible characters will be represented by a vector of length $n$, containing $0$ expect for a $1$ in position $i$. Following our previous examples, ```a = [1, 0, 0]``` and ```b = [0, 1, 0]```.\n",
    "- Hence, a sequence of characters is a list of one-hot vectors. Our goal will be to predict, given an input sequence of fixed length (here, this length is given by ```maximum_seq_length```) the next character. Hence, we need to build two lists: ```sentences```, containing the input sequences, and ```next_char``` the characters to be predicted.\n",
    "- We do not necessarily need to take all possible sequences. We can select one every ```time_step``` steps.\n",
    "\n",
    "Example: Using the previous dictionnaries, the sequence:\n",
    "```'acabbaccaabba'``` with ```maximum_seq_length = 4``` and ```time_step = 2``` would give the following lists:\n",
    "\n",
    "```python\n",
    "sentences = ['acab', 'abba', 'bacc', 'ccaa', 'aabb']\n",
    "```\n",
    "\n",
    "```python\n",
    "next_char = ['b', 'c', 'a', 'b', 'a']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIIvHDZvfHn7",
    "outputId": "dbf65c75-d398-49a9-a146-e51b4263b0de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sequences: 160479\n"
     ]
    }
   ],
   "source": [
    "maximum_seq_length = 24\n",
    "time_step = 1\n",
    "sentences = []\n",
    "next_char = []\n",
    "for i in range(0, len(text) - maximum_seq_length, time_step):\n",
    "    sentences.append(text[i: i + maximum_seq_length])\n",
    "    next_char.append(text[i + maximum_seq_length])\n",
    "print('Number of Sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UYa-53Qqfcac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7z7a6_7femM"
   },
   "source": [
    "#### Creating training tensors\n",
    "- We need to transform these lists into tensors, using one-hot vectors to represent characters.\n",
    "- We will need 3 dimensions for the training examples from ```sentences```: the number of examples, the length of the sequence, and the dimension of the one-hot vector\n",
    "- This is reduced to 2 dimensions for the ```next_char```: number of examples and one-hot vector.\n",
    "\n",
    "Example: the previous ```sentences``` would become:\n",
    "\n",
    "a : 0, b : 1, c: 2\n",
    "\n",
    "exemple : premier correspond à la sequence acab\n",
    "\n",
    "\n",
    "taille de x : nbre(n_gram) * n-1 * taille du vocabulaire\n",
    "\n",
    "cas des fleurs du mal : taille_x = 160479 * 24 * 92\n",
    "taille_y = 160479 * 92\n",
    "\n",
    "```python\n",
    "X = [[[1, 0, 0],\n",
    "      [0, 0, 1],\n",
    "      [1, 0, 0],\n",
    "      [0, 1, 0]],\n",
    "     [[1, 0, 0],\n",
    "      [0, 1, 0],\n",
    "      [0, 1, 0],\n",
    "      [1, 0, 0]],\n",
    "     [[0, 1, 0],\n",
    "      [1, 0, 0],\n",
    "      [0, 0, 1],\n",
    "      [0, 0, 1]],\n",
    "     [[0, 0, 1],\n",
    "      [0, 0, 1],\n",
    "      [1, 0, 0],\n",
    "      [1, 0, 0]],\n",
    "     [[1, 0, 0],\n",
    "      [1, 0, 0],\n",
    "      [0, 1, 0],\n",
    "      [0, 1, 0]]]\n",
    "```\n",
    "       \n",
    "```python\n",
    "y = [[0, 1, 0],\n",
    "     [0, 0, 1],\n",
    "     [1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [1, 0, 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "oPQEWEKnfgAD"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), maximum_seq_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_char[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC6jUjYYjPHj"
   },
   "source": [
    "#### Implement the model\n",
    "In order to implement the model as simply as possible, we will use ```keras```. It allows to create models with only a few lines of code.\n",
    "First, we will create a very simple model based on a **LSTM**, which is a *recurrent* architecture. Note that one the strength of a recurrent architecture is to allow for inputs of varying length - here, to simplify data processing, we will keep a **fixed input size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gGIn8bKEfgGZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s_ywdkTNRtA"
   },
   "source": [
    "We need to create a LSTM model that takes directly out inputs from ```X``` and try to predict one-hot vectors from ```y```.\n",
    "- What are the input and output dimensions ?\n",
    "  - ```X```: size of the dataset $\\times$ maximum sequence length $\\times$ vocabulary size\n",
    "  - ```y```: size of the dataset $\\times$ vocabulary size\n",
    "- The model should be made with a ```LSTM``` layer, and a ```Dense``` layer followed by a softmax activation function. Work out the intermediate dimensions:\n",
    "  - ```X``` $\\rightarrow$ (LSTM) $\\rightarrow$ ```h``` $\\rightarrow$ (Dense) $\\rightarrow$ ```s``` $\\rightarrow$ (softmax) $\\rightarrow$ ```pred```\n",
    "  - Look at layers arguments and find out to proper ```input_shape``` for the ```LSTM``` layer and the proper size for the ```Dense``` layer.\n",
    "  - We can use 256 as the size of hidden states for the ```LSTM```.\n",
    "- We will minimize ```cross-entropy(pred, y)```. Use the ```categorical_crossentropy``` loss, with the optimizer of your preference (for example, ```adam```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gzIRMSx1gJRh"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maximum_seq_length, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slRjcDrkNZ7I"
   },
   "source": [
    "We will now only need a few functions to use this model:\n",
    "- ```model.fit```, which you will call on the appropriately processed data ```X, y```\n",
    "- ```model.predict```, which we will use on an input **of the same dimension of X** to output the probabilities. That includes the *first one*, corresponding to the number of examples in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XguFAVGjnVG"
   },
   "source": [
    "#### Create a function to generate text with our model\n",
    "- We use the output of our model to select the next most probable character (with the ```argmax``` function)\n",
    "- We need to transform an input text into an input tensor, as before (taking the right length, the last ```maximum_seq_length``` characters)\n",
    "- We need to transform back the most probable index into a character and add it to our text.\n",
    "- This must be looped ```num_generated``` times, each time obtaining a new input tensor from the new input sequence (which has the character we previously predicted at the end !)\n",
    "\n",
    "\n",
    "We can begin by writing a function facilitating the transfer between text and tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-LIS3MhQOcSX"
   },
   "outputs": [],
   "source": [
    "def get_tensor(sentence, maximum_seq_length, voc):\n",
    "    x = np.zeros((1, maximum_seq_length, len(voc)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[0, t, voc[char]] = 1.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBhnsrCWOc-q"
   },
   "source": [
    "The following function (```end_epoch_generate```) is here to facilitate automatic generation at the end of each epoch, so you can monitor of generation changes as the model trains. It calls the ```generate_next``` function upon each sequence of text in ```texts_ex```. The only element in this list right now comes from the training data - you can add your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "C-U3odSfkS70"
   },
   "outputs": [],
   "source": [
    "def generate_next(model, text, num_generated=120):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = get_tensor(sentence, maximum_seq_length, char_indices)\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        next_index = np.argmax(predictions)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)\n",
    "\n",
    "def end_epoch_generate(epoch, _):\n",
    "    print('\\n Generating text after epoch: %d' % (epoch+1))\n",
    "    texts_ex = [\"La sottise, l'erreur, le péché\"]\n",
    "    for text in texts_ex:\n",
    "        sample = generate_next(model, text)\n",
    "        print('%s' % (sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "yC7yZ4LZkTBj",
    "outputId": "15ef6ffd-4dde-430e-c209-57e44e3d7261"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"La sottise, l'erreur, le péchéc«w«--T-T--T--T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T---T-\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ex = \"La sottise, l'erreur, le péché\"\n",
    "generate_next(model, text_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7hDj4zvkTEv",
    "outputId": "86a8c29f-5231-4814-fb3e-84c5f18316c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 998/1003 [============================>.] - ETA: 0s - loss: 2.5212\n",
      " Generating text after epoch: 1\n",
      "La sottise, l'erreur, le péchése de le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le \n",
      "1003/1003 [==============================] - 20s 16ms/step - loss: 2.5191 - val_loss: 2.1420\n",
      "Epoch 2/10\n",
      "1003/1003 [==============================] - ETA: 0s - loss: 2.0465\n",
      " Generating text after epoch: 2\n",
      "La sottise, l'erreur, le péchés de le chait de le chait de le chait de le chait de le chait de le chait de le chait de le chait de le chait de le chai\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 2.0465 - val_loss: 1.9613\n",
      "Epoch 3/10\n",
      " 999/1003 [============================>.] - ETA: 0s - loss: 1.9130\n",
      " Generating text after epoch: 3\n",
      "La sottise, l'erreur, le péchés de la coure et le coure et le coure et le coure et le coure et le coure et le coure et le coure et le coure et le cour\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.9130 - val_loss: 1.8633\n",
      "Epoch 4/10\n",
      "1001/1003 [============================>.] - ETA: 0s - loss: 1.8197\n",
      " Generating text after epoch: 4\n",
      "La sottise, l'erreur, le péchés de la grand de la grand de la grand de la grand de la grand de la grand de la grand de la grand de la grand de la gran\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.8196 - val_loss: 1.7894\n",
      "Epoch 5/10\n",
      " 998/1003 [============================>.] - ETA: 0s - loss: 1.7410\n",
      " Generating text after epoch: 5\n",
      "La sottise, l'erreur, le péchété de la grande la mort de la grande la mort de la grande la mort de la grande la mort de la grande la mort de la grande\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.7409 - val_loss: 1.7299\n",
      "Epoch 6/10\n",
      "1003/1003 [==============================] - ETA: 0s - loss: 1.6794\n",
      " Generating text after epoch: 6\n",
      "La sottise, l'erreur, le péchére et la morte,\n",
      "   Et que l'air son coeur de la main de la main de la main de la main de la main de la main de la main d\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.6794 - val_loss: 1.6893\n",
      "Epoch 7/10\n",
      "1001/1003 [============================>.] - ETA: 0s - loss: 1.6231\n",
      " Generating text after epoch: 7\n",
      "La sottise, l'erreur, le péché de la morte\n",
      "       Et que le soir de la morte\n",
      "       De la grand de la morte et le morte\n",
      "       De la grand de la morte\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.6231 - val_loss: 1.6518\n",
      "Epoch 8/10\n",
      "1001/1003 [============================>.] - ETA: 0s - loss: 1.5731\n",
      " Generating text after epoch: 8\n",
      "La sottise, l'erreur, le péché de mon coeur de l'arme des morts de l'arme des morts de l'arme des morts de l'arme des morts de l'arme des morts de l'a\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.5728 - val_loss: 1.6304\n",
      "Epoch 9/10\n",
      " 994/1003 [============================>.] - ETA: 0s - loss: 1.5264\n",
      " Generating text after epoch: 9\n",
      "La sottise, l'erreur, le péchéré de la mainte et le des pleurs de la mainte et le des pleurs de la mainte et le des pleurs de la mainte et le des pleu\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.5260 - val_loss: 1.6061\n",
      "Epoch 10/10\n",
      " 996/1003 [============================>.] - ETA: 0s - loss: 1.4806\n",
      " Generating text after epoch: 10\n",
      "La sottise, l'erreur, le péché de l'ange par les plaisirs de l'artière,\n",
      "   Sous les plaisirs de la plus de ces plaisirs de l'artière,\n",
      "   Sous les plai\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.4808 - val_loss: 1.6006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7aaa1810fd90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72FpUqWun0WI"
   },
   "source": [
    "#### Using character embeddings\n",
    "- Instead of using one-hot vectors to represent characters, we will now use character embeddings, which are vectors belonging to the same space.\n",
    "- We will need as many vectors as there is characters. The input of the network will be simpler, since we will just need to indicate to the model which character is in input.\n",
    "- The output does not change: indeed, Keras uses one-hot vectors for the target of the categorical cross-entropy loss.\n",
    "Example: the previous example ```sentences``` would now become:\n",
    "- We need to add a ```Embedding``` layer to the model, with the right input size, and to choose which dimension use for our embeddings.\n",
    "\n",
    "```python\n",
    "X = [[0, 2, 0, 1],\n",
    "     [0, 1, 1, 0],\n",
    "     [1, 0, 2, 2],\n",
    "     [2, 2, 0, 0],\n",
    "     [0, 0, 1, 1]]\n",
    "```\n",
    "       \n",
    "```python\n",
    "y = [[0, 1, 0],\n",
    "     [0, 0, 1],\n",
    "     [1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [1, 0, 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "myi-eu8cuJxF"
   },
   "outputs": [],
   "source": [
    "X_emb = np.zeros((len(sentences), maximum_seq_length), dtype=int)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X_emb[i, t] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_2QkLn7itmIF"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model_emb = Sequential()\n",
    "model_emb.add(Embedding(len(chars), 32, input_length = maximum_seq_length))\n",
    "model_emb.add(LSTM(256))\n",
    "model_emb.add(Dense(len(chars)))\n",
    "model_emb.add(Activation('softmax'))\n",
    "\n",
    "model_emb.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "J5l1G2-k9kP3"
   },
   "outputs": [],
   "source": [
    "def generate_next(model, text, num_generated=120):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = np.zeros((1, maximum_seq_length))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t] = char_indices[char]\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        next_index = np.argmax(predictions)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)\n",
    "\n",
    "def end_epoch_generate(epoch, _):\n",
    "    print('\\n Generating text after epoch: %d' % (epoch+1))\n",
    "    texts_ex = [\"La sottise, l'erreur, le péché\"]\n",
    "    for text in texts_ex:\n",
    "        sample = generate_next(model_emb, text)\n",
    "        print('%s' % (sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sK70WaUoltY",
    "outputId": "f9e0eb5f-8d09-4931-b5fb-1b9363d518f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 998/1003 [============================>.] - ETA: 0s - loss: 2.4975\n",
      " Generating text after epoch: 1\n",
      "La sottise, l'erreur, le péchése de poure de poure de poure de poure de poure de poure de poure de poure de poure de poure de poure de poure de poure \n",
      "1003/1003 [==============================] - 21s 18ms/step - loss: 2.4956 - val_loss: 2.1160\n",
      "Epoch 2/10\n",
      "1000/1003 [============================>.] - ETA: 0s - loss: 2.0289\n",
      " Generating text after epoch: 2\n",
      "La sottise, l'erreur, le péchéte et les les les les les les les les les les les les les les les les les les les les les les les les les les les les le\n",
      "1003/1003 [==============================] - 14s 14ms/step - loss: 2.0283 - val_loss: 1.9280\n",
      "Epoch 3/10\n",
      "1000/1003 [============================>.] - ETA: 0s - loss: 1.8804\n",
      " Generating text after epoch: 3\n",
      "La sottise, l'erreur, le péchéte et de sour de sour de sour de sour de sour de sour de sour de sour de sour de sour de sour de sour de sour de sour de\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.8801 - val_loss: 1.8335\n",
      "Epoch 4/10\n",
      " 999/1003 [============================>.] - ETA: 0s - loss: 1.7810\n",
      " Generating text after epoch: 4\n",
      "La sottise, l'erreur, le péchét de le son coeur de le son coeur de le son coeur de le son coeur de le son coeur de le son coeur de le son coeur de le \n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.7807 - val_loss: 1.7448\n",
      "Epoch 5/10\n",
      "1002/1003 [============================>.] - ETA: 0s - loss: 1.7056\n",
      " Generating text after epoch: 5\n",
      "La sottise, l'erreur, le péchés de la plaire et les chasses des morts son son et les chasses des morts son son et les chasses des morts son son et les\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.7056 - val_loss: 1.7026\n",
      "Epoch 6/10\n",
      " 995/1003 [============================>.] - ETA: 0s - loss: 1.6430\n",
      " Generating text after epoch: 6\n",
      "La sottise, l'erreur, le péché de la coeur de la coeur de la coeur de la coeur de la coeur de la coeur de la coeur de la coeur de la coeur de la coeur\n",
      "1003/1003 [==============================] - 15s 15ms/step - loss: 1.6431 - val_loss: 1.6520\n",
      "Epoch 7/10\n",
      " 996/1003 [============================>.] - ETA: 0s - loss: 1.5904\n",
      " Generating text after epoch: 7\n",
      "La sottise, l'erreur, le péchés de la plus de la plus de la plus de la plus de la plus de la plus de la plus de la plus de la plus de la plus de la pl\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.5903 - val_loss: 1.6205\n",
      "Epoch 8/10\n",
      "1002/1003 [============================>.] - ETA: 0s - loss: 1.5435\n",
      " Generating text after epoch: 8\n",
      "La sottise, l'erreur, le péché de la morte et le coeur de la morte et le coeur de la morte et le coeur de la morte et le coeur de la morte et le coeur\n",
      "1003/1003 [==============================] - 14s 14ms/step - loss: 1.5435 - val_loss: 1.5967\n",
      "Epoch 9/10\n",
      " 998/1003 [============================>.] - ETA: 0s - loss: 1.5016\n",
      " Generating text after epoch: 9\n",
      "La sottise, l'erreur, le péché de l'amour de la bouche et le soleil de la morte et de la morte et de la morte et de la morte et de la morte et de la m\n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.5019 - val_loss: 1.5729\n",
      "Epoch 10/10\n",
      "1000/1003 [============================>.] - ETA: 0s - loss: 1.4610\n",
      " Generating text after epoch: 10\n",
      "La sottise, l'erreur, le péchés de la morte et la courtise et les plus de la morte et le coeur de la morte et la conuse et les plus des plus des plus \n",
      "1003/1003 [==============================] - 13s 13ms/step - loss: 1.4611 - val_loss: 1.5515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7aa9f0775f30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb.fit(X_emb, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ATJnocork5a"
   },
   "source": [
    "#### Sampling with our model\n",
    "- Now, instead of simply selecting the most probable next character, we would like to be able to draw a sample from the distribution output by the model.\n",
    "- To better control the generation, we would like to use the argument ```temperature```, to smooth the distribution.\n",
    "- We will use the ```multinomial``` function from the ```random``` package to draw samples.\n",
    "- We integrate this into a function ```generate_sample``` that is almost exactly like ```generate_next```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "psIfwKiLol3W"
   },
   "outputs": [],
   "source": [
    "def reweight(predictions, temperature):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    log_predictions = np.log(predictions) / temperature\n",
    "    predictions = np.exp(log_predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    return predictions\n",
    "\n",
    "def sample(predictions, temperature):\n",
    "    predictions = reweight(predictions, temperature)\n",
    "    sampled = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(sampled)\n",
    "\n",
    "def generate_sample(model, text, num_generated=120, temperature=1.0):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = np.zeros((1, maximum_seq_length))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t] = char_indices[char]\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(predictions, temperature)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hy5z2vW1kTHy",
    "outputId": "f8393f84-ff69-4ed5-dae8-c15ec55c17c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La sottise, l'erreur, le péchés était de voix, qui noire aux anges,\n",
      "       De ton vieil leurs du souviens.\n",
      " \n",
      "   Au blatant son trouvent comment des fa\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(model_emb, text_ex, temperature = 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "U_wD4v8Qy-nG"
   },
   "outputs": [],
   "source": [
    "def sample_top_k(predictions, temperature, k):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    log_predictions = np.log(predictions) / temperature\n",
    "    indices_to_remove = log_predictions.argsort()[:k]\n",
    "    log_predictions[indices_to_remove] = -float('Inf')\n",
    "    predictions = np.exp(log_predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_sample_top_k(model, text, num_generated=120, temperature=1.0, k=10):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = np.zeros((1, maximum_seq_length))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t] = char_indices[char]\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample_top_k(predictions, temperature, k)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hXKdtloy-rm",
    "outputId": "e3cf1ebe-389f-49e1-cb03-af1a08bb9c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La sottise, l'erreur, le péchément la gardie,\n",
      "   Doucement le somme la ros.\n",
      " \n",
      "   Elle se plâcre armorreux;\n",
      "   Nous gressé tout comme un dit:\n",
      "   Mortis\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample_top_k(model_emb, text_ex, temperature = 0.8, k = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "qDWCo3GczC9j"
   },
   "outputs": [],
   "source": [
    "def sample_top_p(predictions, temperature, p):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    log_predictions = np.log(predictions) / temperature\n",
    "    predictions = np.exp(log_predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "\n",
    "    cum_prob = 0.0\n",
    "    incr = 0\n",
    "    indices = predictions.argsort()\n",
    "    probs = predictions[indices]\n",
    "    while cum_prob < p:\n",
    "        cum_prob += probs[incr]\n",
    "        incr += 1\n",
    "    indices_to_remove = indices[incr:]\n",
    "\n",
    "    log_predictions[indices_to_remove] = -float('Inf')\n",
    "    predictions = np.exp(log_predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    sampled = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(sampled)\n",
    "\n",
    "\n",
    "def generate_sample_top_p(model, text, num_generated=60, temperature=1.0, p=0.9):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = np.zeros((1, maximum_seq_length))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t] = char_indices[char]\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample_top_p(predictions, temperature, p)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UanOyXX8zDAQ",
    "outputId": "665bf6f1-b468-4397-fb08-31b138a16270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la sottise, l'erreur, le péché,\n",
      "   Car ta ferce ou de beauté,\n",
      "   Comme allonge parfim d'un\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample_top_p(model_emb, text_ex.lower(), temperature = 0.7, p = 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa1z4yJzrvMp"
   },
   "source": [
    "#### Generate text with the beam algorithm\n",
    "- We need to loop for each character we want to generate, keeping track of the best ```beam_size``` sequences at the most.\n",
    "- Besides keeping track of past generated character for each of these ```beam_size``` sequences, we need to keep track of their log-probability.\n",
    "- This is done by, at each loop, keeping the ```beam_size```best predictions for each of the ```beam_size``` sequences, computing the log-probabilities of the newly formed (```beam_size```)$^2$ , and keeping the overall ```beam_size``` best new sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "5FTEE4Aqz55B"
   },
   "outputs": [],
   "source": [
    "def generate_beam(model, text, beam_size=16, num_generated=128):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    # Initialization of the beam with log-probabilities for the sequence\n",
    "    current_beam = [(0, [], sentence)]\n",
    "\n",
    "    for l in range(num_generated):\n",
    "        all_beams = []\n",
    "        for prob, current_preds, current_input in current_beam:\n",
    "            x = np.zeros((1, maximum_seq_length))\n",
    "            for t, char in enumerate(current_input):\n",
    "                x[0, t] = char_indices[char]\n",
    "            prediction = model.predict(x = x, verbose = 0)[0]\n",
    "            possible_next_chars = prediction.argsort()[-beam_size:][::-1]\n",
    "            all_beams += [\n",
    "                (prob + np.log(prediction[next_index]),\n",
    "                 current_preds + [next_index],\n",
    "                 current_input[1:] + indices_char[next_index]\n",
    "                )\n",
    "                for next_index in possible_next_chars]\n",
    "\n",
    "        current_beam = sorted(all_beams)[-beam_size:]\n",
    "\n",
    "    return text + ''.join([indices_char[idx] for idx in current_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rk0YnN8mz6Cs",
    "outputId": "72069614-52a8-421a-efa0-ad4832ad2d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La sottise, l'erreur, le péchés,\n",
      "   Comme un coeur de ton coeur.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   LE VIN\n",
      " \n",
      " \n",
      "   Comme une coeur de ton coeur,\n",
      "   Comme une coeur de ton coeur!\n",
      " \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(generate_beam(model_emb, text_ex))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
